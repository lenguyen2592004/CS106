{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YczE55EXiYx9",
        "outputId": "1de18b8d-75c0-4864-f82b-ba410e84c15f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhSyhfEy4XSD"
      },
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import time\n",
        "from IPython import display"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHf1dAVKAcZm"
      },
      "source": [
        "env = gym.make('FrozenLake-v1', render_mode=\"ansi\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-6usoQHAmqh",
        "outputId": "60c1e6e5-b47f-4823-cfb6-3eaa45c2f7af"
      },
      "source": [
        "env.P[0][3] # Transition model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.P to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.P` for environment variables or `env.get_wrapper_attr('P')` that will search the reminding wrappers.\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.3333333333333333, 1, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False),\n",
              " (0.3333333333333333, 0, 0.0, False)]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wh7Su0h0AqQz",
        "outputId": "3cdb20bb-9bb1-462a-d965-3f37ff3c3625"
      },
      "source": [
        "env.observation_space.n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ68w5bpBScC",
        "outputId": "239ec172-f9b7-4b67-fa2a-bc27c143074e"
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWLnvY7VBvIZ"
      },
      "source": [
        "def play(env, policy, render=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = policy[state]\n",
        "        next_state, reward, done, info, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if render:\n",
        "            print(env.render())\n",
        "            time.sleep(0.5)\n",
        "            if not done:\n",
        "                display.clear_output(wait=True)\n",
        "        state = next_state\n",
        "\n",
        "    return (total_reward, steps)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcuDDx6rC5YE",
        "outputId": "10917407-9302-4a8e-f2a9-0ad06b760c17"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play(env, policy_0, True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ1CJNPhDGPA",
        "outputId": "f2910cea-6a77-42e9-c2c3-222c39734fb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 22)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdyjjtGZC9NX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73e93764-4287-4a20-fa6c-14cbafe94f84"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play(env, policy_1, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "\u001b[41mH\u001b[0mFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt0VhyMuDasc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7396d9-2bbc-4ccd-f454-61b01f0c4af7"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play(env, policy_2, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "FHF\u001b[41mH\u001b[0m\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0, 18)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hp6qhRFJDxWR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66cbc27f-cfca-4167-8a85-598d063a919c"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play(env, policy_3, True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU8Q1qMxD6Po"
      },
      "source": [
        "def play_multiple_times(env, policy, max_episodes):\n",
        "    success = 0\n",
        "    list_of_steps = []\n",
        "    for i in range(max_episodes):\n",
        "        total_reward, steps = play(env, policy)\n",
        "\n",
        "        if total_reward > 0:\n",
        "            success += 1\n",
        "            list_of_steps.append(steps)\n",
        "\n",
        "    print(f'Number of successes: {success}/{max_episodes}')\n",
        "    print(f'Average number of steps: {np.mean(list_of_steps)}')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G427z17PEmjQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7c5e179-9170-4708-d83f-3bb74141f073"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "play_multiple_times(env, policy_0, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 0/1000\n",
            "Average number of steps: nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1bkhaFdDmj_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc157ece-e202-482f-9002-13c19379fd58"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "play_multiple_times(env, policy_1, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 49/1000\n",
            "Average number of steps: 10.959183673469388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZYhsb_VEtuR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d0c8d7-538c-4a57-e877-2a0bb8b08873"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "play_multiple_times(env, policy_2, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 105/1000\n",
            "Average number of steps: 15.17142857142857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvvHdMesEzTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc41bf19-febc-4f4f-eb17-478bfa0931f2"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "play_multiple_times(env, policy_3, 1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of successes: 778/1000\n",
            "Average number of steps: 45.20822622107969\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSomNpxJE5lP"
      },
      "source": [
        "def policy_evaluation(env, policy, max_iters=500, gamma=0.9):\n",
        "    # Initialize the values of all states to be 0\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # Update the value of each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            action = policy[state]\n",
        "\n",
        "            # Compute the q-value of the action\n",
        "            q_value = 0\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "            v_values[state] = q_value # update v-value\n",
        "\n",
        "        # Check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A7IhqEOgGkQX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf42449e-2112-46f7-84e0-36e3251e97ba"
      },
      "source": [
        "policy_0 = np.asarray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
        "v_values_0 = policy_evaluation(env, policy_0)\n",
        "print(v_values_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 0-th iteration.\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMjJKI3GGrsN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56cb647e-6754-4462-c93c-cd403848cc07"
      },
      "source": [
        "policy_1 = np.asarray([0, 1, 1, 3, 1, 0, 2, 0, 1, 1, 2, 2, 3, 3, 1, 0])\n",
        "v_values_1 = policy_evaluation(env, policy_1)\n",
        "print(v_values_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 48-th iteration.\n",
            "[0.01904157 0.01519815 0.03161906 0.02371389 0.02538879 0.\n",
            " 0.06648515 0.         0.05924054 0.13822794 0.18999823 0.\n",
            " 0.         0.21152109 0.56684236 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-26M77nEfcV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dc5fcc4-516a-4410-cd12-3e5b6a966b23"
      },
      "source": [
        "np.all(v_values_1 >= v_values_0)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l49O1N8QG0S2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "288e3a50-16cd-43ee-9fdf-8d270bdc06bc"
      },
      "source": [
        "policy_2 = np.array([1, 1, 1, 3, 0, 1, 2, 3, 1, 1, 2, 3, 2, 2, 1, 3])\n",
        "v_values_2 = policy_evaluation(env, policy_2)\n",
        "print(v_values_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 53-th iteration.\n",
            "[0.02889625 0.01951972 0.03616977 0.0271268  0.04790519 0.\n",
            " 0.07391985 0.         0.08288277 0.19339319 0.21022995 0.\n",
            " 0.         0.35153135 0.62684674 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22pRvreGE3Yt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44c73671-9328-492b-80b5-179472dbe9d9"
      },
      "source": [
        "np.all(v_values_2 >= v_values_1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTYYFq6BEXDd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f7b2d6-20e7-4c3b-ae15-c3465d362866"
      },
      "source": [
        "policy_3 = np.array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0])\n",
        "v_values_3 = policy_evaluation(env, policy_3)\n",
        "print(v_values_3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 80-th iteration.\n",
            "[0.06888666 0.06141097 0.07440714 0.05580443 0.09185068 0.\n",
            " 0.11220679 0.         0.14543323 0.24749485 0.29961611 0.\n",
            " 0.         0.37993438 0.63901935 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcEfU3NYE7xN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f9b24d2-d46f-4a0d-d6bd-4e77d28240fb"
      },
      "source": [
        "np.all(v_values_3 >= v_values_2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uh4akjMSHJBF"
      },
      "source": [
        "def value_iteration(env, max_iters=500, gamma=0.9):\n",
        "    # initialize\n",
        "    v_values = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "\n",
        "    for i in range(max_iters):\n",
        "        prev_v_values = np.copy(v_values)\n",
        "\n",
        "        # update the v-value for each state\n",
        "        for state in range(env.observation_space.n):\n",
        "            q_values = []\n",
        "\n",
        "            # compute the q-value for each action that we can perform at the state\n",
        "            for action in range(env.action_space.n):\n",
        "                q_value = 0\n",
        "                # loop through each possible outcome\n",
        "                for prob, next_state, reward, done in env.P[state][action]:\n",
        "                    q_value += prob * (reward + gamma * prev_v_values[next_state])\n",
        "\n",
        "                q_values.append(q_value)\n",
        "\n",
        "            # select the max q-values\n",
        "            best_action = np.argmax(q_values)\n",
        "            v_values[state] = q_values[best_action]\n",
        "            policy[state] = best_action  # save the best action in the policy\n",
        "\n",
        "        # check convergence\n",
        "        if np.all(np.isclose(v_values, prev_v_values)):\n",
        "            print(f'Converged at {i}-th iteration.')\n",
        "            break\n",
        "\n",
        "    return v_values, policy"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r8xAljw7VuMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e92a2fc3-721a-4ffc-e131-2fd7397efd29"
      },
      "source": [
        "optimal_v_values = value_iteration(env, max_iters=500, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at 79-th iteration.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7g9VA3lV2WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70380bee-0a98-449d-be5b-dc7773acf526"
      },
      "source": [
        "optimal_v_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([0.06888615, 0.06141054, 0.07440682, 0.05580409, 0.09185022,\n",
              "        0.        , 0.11220663, 0.        , 0.14543286, 0.2474946 ,\n",
              "        0.29961593, 0.        , 0.        , 0.3799342 , 0.63901926,\n",
              "        0.        ]),\n",
              " array([0, 3, 0, 3, 0, 0, 0, 0, 3, 1, 0, 0, 0, 2, 1, 0]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb0an7gaV39e"
      },
      "source": [
        "def policy_extraction(env, v_values, gamma=0.9):\n",
        "    # initialize\n",
        "    policy = np.zeros(env.observation_space.n, dtype=np.int32)\n",
        "\n",
        "    # loop through each state in the environment\n",
        "    for state in range(env.observation_space.n):\n",
        "        q_values = []\n",
        "        # loop through each action\n",
        "        for action in range(env.action_space.n):\n",
        "            q_value = 0\n",
        "            # loop each possible outcome\n",
        "            for prob, next_state, reward, done in env.P[state][action]:\n",
        "                q_value += prob * (reward + gamma * v_values[next_state])\n",
        "\n",
        "            q_values.append(q_value)\n",
        "\n",
        "        # select the best action\n",
        "        best_action = np.argmax(q_values)\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return policy"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TGCF4G7XErH"
      },
      "source": [
        "optimal_policy = policy_extraction(env, optimal_v_values, gamma=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_policy"
      ],
      "metadata": {
        "id": "qkHYtfm4qikV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play(env, optimal_policy, True)"
      ],
      "metadata": {
        "id": "1Ww12Uh5qCUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-m4ZqWZXKqG"
      },
      "source": [
        "play_multiple_times(env, optimal_policy, 1000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "\n",
        "def policy_iteration(env, gamma=0.9, max_iters=500):\n",
        "    # initialize a random policy or all zeros policy\n",
        "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
        "    for i in range(max_iters):\n",
        "        # evaluate the current policy\n",
        "        v_values = policy_evaluation(env, policy, max_iters, gamma)\n",
        "\n",
        "        # extract a new policy based on the updated value function\n",
        "        new_policy = policy_extraction(env, v_values, gamma)\n",
        "\n",
        "        # check if the new policy is the same as the old policy\n",
        "        if np.all(new_policy == policy):\n",
        "            break\n",
        "\n",
        "        policy = new_policy\n",
        "\n",
        "    return policy, v_values\n",
        "\n",
        "# # create the environment\n",
        "# env = gym.make('FrozenLake-v1', render_mode=\"ansi\")\n",
        "\n",
        "# # run the policy iteration\n",
        "# optimal_policy, optimal_v_values = policy_iteration(env)\n",
        "# print(\"Optimal Policy:\", optimal_policy)\n",
        "# print(\"Optimal Value Function:\", optimal_v_values)"
      ],
      "metadata": {
        "id": "YOQ7Hs4DqX2T"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "environments = ['FrozenLake-v1', 'FrozenLake8x8-v1', 'Taxi-v3']\n"
      ],
      "metadata": {
        "id": "BMiFWyV1x3Wc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "i='FrozenLake-v1'\n",
        "s_policy_iteration = 0\n",
        "t_i = time.perf_counter()\n",
        "env = gym.make(i, render_mode=\"ansi\")\n",
        "optimal_policy, optimal_v_values = policy_iteration(env)\n",
        "print(play(env, optimal_policy, True))\n",
        "t_f = time.perf_counter()\n",
        "s_policy_iteration += t_f - t_i\n",
        "print(\"Thời gian chạy:\",t_f - t_i,\"s\")\n"
      ],
      "metadata": {
        "id": "81cQP9sehKMz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "867846a3-c5b8-46a6-8970-1e59ca68b0b8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "(1.0, 50)\n",
            "Thời gian chạy: 25.200034587000005 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "environments = ['FrozenLake-v1', 'FrozenLake8x8-v1', 'Taxi-v3']\n",
        "i='FrozenLake-v1'\n",
        "s_value_iteration = 0\n",
        "t_i = time.perf_counter()\n",
        "env = gym.make(i, render_mode=\"ansi\")\n",
        "optimal_value, policy = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "print(play(env, policy, True))\n",
        "t_f = time.perf_counter()\n",
        "s_value_iteration += t_f - t_i\n",
        "print(f\"Thời gian chạy:{t_f - t_i}s\")\n"
      ],
      "metadata": {
        "id": "5AfKWx7Xp0p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "722e1884-3ba9-4770-a878-e7c8ab6d8b44"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFF\u001b[41mG\u001b[0m\n",
            "\n",
            "(1.0, 42)\n",
            "Thời gian chạy:21.203350549999982s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i='FrozenLake8x8-v1'\n",
        "s_policy_iteration = 0\n",
        "t_i = time.perf_counter()\n",
        "env = gym.make(i, render_mode=\"ansi\")\n",
        "optimal_policy, optimal_v_values = policy_iteration(env)\n",
        "print(play(env, optimal_policy, True))\n",
        "t_f = time.perf_counter()\n",
        "s_policy_iteration += t_f - t_i\n",
        "print(\"Thời gian chạy:\",t_f - t_i,\"s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M3eCypS5y23w",
        "outputId": "196959e4-8756-4a19-c7b4-2e5a54a7ca35"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Down)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFF\u001b[41mH\u001b[0mFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFFG\n",
            "\n",
            "(0.0, 22)\n",
            "Thời gian chạy: 12.517198558000018 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i='FrozenLake8x8-v1'\n",
        "s_value_iteration = 0\n",
        "t_i = time.perf_counter()\n",
        "env = gym.make(i, render_mode=\"ansi\")\n",
        "optimal_value, policy = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "print(play(env, policy, True))\n",
        "t_f = time.perf_counter()\n",
        "s_value_iteration += t_f - t_i\n",
        "print(f\"Thời gian chạy:{t_f - t_i}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "exxJenIszwDQ",
        "outputId": "88e865d9-a5c2-4290-ab7a-50fcf9a424e9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFFFFFF\n",
            "FFFFFFFF\n",
            "FFFHFFFF\n",
            "FFFFFHFF\n",
            "FFFHFFFF\n",
            "FHHFFFHF\n",
            "FHFFHFHF\n",
            "FFFHFFF\u001b[41mG\u001b[0m\n",
            "\n",
            "(1.0, 62)\n",
            "Thời gian chạy:31.64873072799992s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i='Taxi-v3'\n",
        "s_policy_iteration = 0\n",
        "t_i = time.perf_counter()\n",
        "env = gym.make(i, render_mode=\"ansi\")\n",
        "optimal_policy, optimal_v_values = policy_iteration(env)\n",
        "print(play(env, optimal_policy, True))\n",
        "t_f = time.perf_counter()\n",
        "s_policy_iteration += t_f - t_i\n",
        "print(\"Thời gian chạy:\",t_f - t_i,\"s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABHvE6krzLKY",
        "outputId": "64ad8ede-4844-4875-e4ec-347b0e817087"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "(7, 14)\n",
            "Thời gian chạy: 21.855840166999997 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "i='Taxi-v3'\n",
        "s_value_iteration = 0\n",
        "t_i = time.perf_counter()\n",
        "env = gym.make(i, render_mode=\"ansi\")\n",
        "optimal_value, policy = value_iteration(env, max_iters=500, gamma=0.9)\n",
        "print(play(env, policy, True))\n",
        "t_f = time.perf_counter()\n",
        "s_value_iteration += t_f - t_i\n",
        "print(f\"Thời gian chạy:{t_f - t_i}s\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWZupSORzwoK",
        "outputId": "c296ec6d-d0ec-46fb-a074-fab0d2b73310"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "(5, 16)\n",
            "Thời gian chạy:13.550028194999982s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Các em có nhận xét gì so sánh Policy Iteration và Value Iteration?\n",
        "# Trong các toy games ở đây thì thuật toán nào cho ra kết quả nhanh hơn (trung bình thời gian chạy)?\n",
        "\n",
        "# Viết ngắn gọn nhận xét và kết quả (dạng comment) vào file ipynb Jupyter notebook."
      ],
      "metadata": {
        "id": "daX2jCm1jH5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# nhận xét gì so sánh Policy Iteration và Value Iteration?\n",
        "# Policy Iteration:\n",
        "# - Gồm 2 bước chính : policy evaluation and policy improvement.\n",
        "# - Thay thế giữa việc đánh giá 1 policy nhất định và cải thiện nó bằng cách làm cho nó trở nên tham lam đối với hàm giá trị của nó.\n",
        "# - Đạt được sự hội tụ khi policy không thay đổi sau một bước extraction,\n",
        "# thường yêu cầu ít lần lặp hơn nhưng mỗi lần lặp lại liên quan đến\n",
        "# việc giải một hệ phương trình (sao lưu toàn bộ chính sách).\n",
        "# - Có thể chậm hơn đối với các môi trường có không gian trạng thái lớn hoặc khi bước đánh giá yêu cầu nhiều lần lặp để hội tụ.\n",
        "# Value Iteration:\n",
        "# - Đơn giản hóa quy trình bằng cách kết hợp policy evaluation và extraction thành một bước duy nhất.\n",
        "# - Tính toán trực tiếp hàm giá trị tối ưu bằng cách xem xét tất cả các hành động ở mỗi trạng thái rồi chọn giá trị tối đa.\n",
        "# - Thường hội tụ nhanh hơn policy iteration về số lần lặp vì nó liên tục cập nhật các ước tính giá trị về giá trị tối ưu.\n",
        "# - Mỗi lần lặp lại có khả năng nhanh hơn vì nó tránh được khả năng hội tụ chậm trong việc policy evalution."
      ],
      "metadata": {
        "id": "YQcmyuh_jbBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trong các toy games ở đây thì thuật toán nào cho ra kết quả nhanh hơn (trung bình thời gian chạy)?\n",
        "\"\"\"\n",
        "Kết quả của các toy games khi lần lượt thực hiện các thuật toán Policy Iteration và Value Iteration.\n",
        "'FrozenLake-v1': 25.200034587000005s(Số điểm đạt được:1.0, số bước:50)(Với Policy Iteration) và 21.203350549999982s(Số điểm đạt được:1.0, số bước:42)(với Value Iteration).\n",
        "'FrozenLake8x8-v1':12.517198558000018s(Số điểm đạt được:0.0, số bước:22)(Với Policy Iteration) và 31.64873072799992s(Số điểm đạt được:1.0, số bước:62)(với Value Iteration).\n",
        "'Taxi-v3':21.855840166999997s(Số điểm đạt được:7, số bước:14)(Với Policy Iteration) và 13.550028194999982s(Số điểm đạt được:5, số bước:16)(với Value Iteration).\n",
        "Nhận xét:\n",
        "-Thuật toán Value Iteration cho ra kết quả nhanh hơn(bằng việc policy evaluation và extraction thành một bước duy nhất, nhờ đó giảm thời gian chạy) ở hầu hết các toy games('FrozenLake-v1' và 'Taxi-v3').\n",
        "-Thuật toán Value Iteration cũng không có kết quả thấp hơn Policy Iteration ở hầu hết các toy games('FrozenLake-v1' và 'FrozenLake8x8-v1').\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "ZCHpj5N4Yq3-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
